\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/scenarios.pdf}
    \caption{Overview of the Scenario Based Emotion and Trust Measurement - We recreate real world traffic situations in simulation using Pre-Scan. During experimentation, we invite human drivers to first manually drive through each scenario (mode 1), followed by an autonomous driving mode where limited feedback and car intentions are provided to the driver, and finally in mode 3, the users experience fully autonomous driving with full feedback and situational awareness provided. We then survey the participants about what feedback enables their trust, which UI elements do they prefer, which mode made them emotionally satisfied.  }
    \label{fig:scenario}
\end{figure}

Behind the wheels of a self-driving car, everyone of a sudden becomes a backseat driver. For thrust 1 on monitoring driver behavior and emotions (Section~\ref{sec:behaviour}, we want to observe and record driver activity in a controlled environment. 
For thrust 3 on feedback design (Section~\ref{sec:feedback}), we want to measure how passengers react to varying degrees of feedback and explanations provided to them by the UI. We setup different traffic scenarios in our full scale driving simulator, to run experiments to gather the data required to build models for human behavior, and test the effectiveness of different kinds of feedback. 
We can simulate multiple scenarios – a four way stop sign, safely passing bicyclists on a narrow road, overtaking a large semi on a freeway, driving too close to a barrier in the absence of a shoulder on the left lane, etc. 

\subsubsection{Participants}
We are proposing to conduct user studies and driving experiments for Thrusts 1, and 3.
The design of a subject study protocol is being undertaken at the time of the witting of the proposal. We are closely working with the Institutional Review Board for Social and Behavioral Sciences at the University of Virginia. The data management plan describes in detail how user data will be recorded, used, distributed, and maintained. A total of at least 100 participants from the University of Virginia will be recruited for participation in this study over 3 years. We we take measures to ensure sufficient diversity within the participation pool. Subjects will be awarded in exchange for participation in accordance with the University of Virginia’s Institutional Review Board practices, and we have requested a budget for the same.


\subsubsection{Setup}
Figure~\ref{fig:scenario} shows the setup of an academic-scale testbed that co-PI Feng's group has partially built and will complete during the proposed effort. The hardware platform is based on the Force Dynamics 401CR driving simulator. This four-axis motion platform can pitch, roll, yaw, and heave, to simulate the experience of being in a vehicle. Thus, we expect to collect data about realistic human response during the driving. The human is interfaced to the hardware platform through PreScan, which is a software tool designed as a development environment for Advanced Driver Assistance Systems (ADAS) and Intelligent Vehicle (IV) systems. PreScan supports virtual sensor technologies such as radar, laser, camera, ultrasonic, GPS and C2C/C2I communications. 
Sensors will be used for both high-level inference of human's intent and preferences and low-level monitoring of human behavioral, mental and physiological states. These sensors include EEG for neural signals, EKG for heart activity, EMG for muscle activity, a camera for head tracking, eye tracking suite and cloud-based speech recognizer.

\subsubsection{Design and procedure}

As shown in Fig.~\ref{fig:scenario}, we have re-created 4 real world driving situations in Pre-Scan - passing bicyclists on a narrow back road, a 4 way intersection, overtaking a semi truck at freeway speeds, and driving on a  freeway in the left lane in the absence of a left shoulder. 
For each of these scenarios, we will conduct the following user experiment:
\begin{enumerate}[itemsep=0pt,parsep=0pt,topsep=4pt,leftmargin=0.4in]
    \item Mode 1 - Manual driving: In this mode, we ask the subject to drive through a given scenario. Each scenario's simulation is a few minutes. While they drive through the scenario, we monitor the person's behavior and emotional cues as well as log their driving behavior. This driving behavior ( steering, brake, acceleration) can be considered as their baseline profile. We also track their gaze, to asses what the driver is paying attention to as they drive. As an example, we will ask the subject to drive through the four way stop sign scenario, where they have to self-asses their right of way as they approach a stop sign. We can then intentionally program one of the agents in the simulation to drive3 out of the order for the right of way and see how the driver adapts. 
    \item Mode 2 - Autonomous driving with limited feedback: Next, we ask the same subject to this time, sit back and relax, as the car drives by itself in the simulator. Everything in the scenario is identical to last time, except that the driving is fully autonomous. We present the driver with a visualization of a virtual dashboard which depicts what the car sees, very similar to what most of the dashboards for semi-autonomous vehicles depict today.  Once again, we monitor the drivers behavioral and emotional cues to interpret their trust in the system. We also give them the option to press a button on the steering wheel, every time they think the car did something that they did not anticipate, or if they mistrusted the car's autonomous driving actions. ``flagging'' these events in a scenario provides us with a basis for designing UI feedback. Continuing with our previous example, a visualization of the four way stop sigh is shown the viewer as feedback, but no indication is given as to if the autonomous car has figured out its right of way, or what is its intended trajectory.
    \item Mode 3 - Autonomous driving with full feedback: Lastly, we have the autonomous car drive through the same scenario one more time, but this time we provide full situational awareness of the scene to the user along with cues about the car's intended actions. In the stop sign example, we project the car's understanding of its right of way, and how that dynamically updates as other vehicles drive through the intersection. We project the cars intended trajectory so the user know that the car will make a left turn and will yield to incoming traffic. More generally, for the parts of the simulation which were flagged by the user, we run the Deep Explanations engine to provide natural language explanations for the car's actions. 
    
\end{enumerate}

Following the experiment, we survey the subjects to understand and gather data on which explanations help increase the trust of the subject, which UI elements work, and to what extent.
Each participant will also be given a questionnaire, adapted from~\cite{merritt2013trust}, to learn about their levels of trust in the autonomous car. 
We will use scales which empirically define the feeling of trust in the autonomous system from~\cite{jian2000foundations} which has been used in many studies about trust~\cite{hoff2015trust}.

\subsubsection{Expected results} 
Upon the completion of data collection, the data will be subjected to a factor analysis to reveal the underlying factor structure of the experimental scale. 
The goal of factor analysis is to condense a larger set of variables to a smaller set of factors, which account for a sizeable proportion of variability within the items. Thus, it is desirable to have a few factors that account for a large portion of the variance. 
A one-way analysis of variance (ANOVA) will also be conducted to examine differences in trust amongst the different driving modes. 
It is anticipated that there will be differences in trust depending on the subject's preferences, driving style, and the autonomous car's behavior. 
More specifically, we anticipate that trust will attenuate with higher levels of feedback. We will create a library of UI elements, a library of simulation scenarios, with detailed statistics about which elements worked and what was the subject's assesment.

In addition, the simulated scenario based experimentation will enable:
\begin{enumerate}[itemsep=0pt,parsep=0pt,topsep=4pt,leftmargin=0.4in]
    \item Creating a library of emotional profiles, and models, of drivers.
    \item Identifying the mapping between the emotional state and the control action of the autonomous car, and vice-versa.
    \item Collect valuable data for training the Deep Explanations networks for natural language feedback.
    \item By deploying the same subject in a real car and monitoring their preferences and driving behavior, we will also validate the realism of the simulation study. 
\end{enumerate}

Although, the simulation setup has a distinct advantage, i.e. repeatability of the same scenario, one of the goals of the proposed work is to develop the proposed models for emotion and driving behavior using data from a real deployment in a car (described later in this section). 


