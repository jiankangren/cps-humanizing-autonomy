%!TEX root = main.tex

\section{Experimentation and Evaluation Plan}\label{sec:exp}
% \textit{This section should describe how the research concepts proposed will be demonstrated and validated. It should present metrics for success. It should identify critical experiments, and describe how the research will be demonstrated, including through simulation, prototyping, and integration with real (including sub-scale) cyber-physical systems. For Medium and Frontier projects, the validation plan must include experimentation on an actual cyber-physical system.}
For each of the proposed research thrusts in Sections~[\ref{sec:behaviour}],[\ref{sec:reachability}], and [\ref{sec:feedback}], we will conduct extensive experimentation 
which are described in detail next.
%using high fidelity automobile simulators like PreScan and AirSim, a full scale diving simulator, smaller scale autonomous vehicle test-beds, and a full scale car fitted with sensors for monitoring human driving behavior and emotions. We next describe in detail, the different automotive cyber-physical systems tesbeds, and a cross cutting experimentation plan. 

\subsection{Scenario-based behavior and trust modeling}
\label{subsec:trust-modeling}
\input{scenarios}

% \begin{figure}[t]
% \centering
% \includegraphics[width=.9\textwidth]{figures/testbed}
% \caption{Our partially built testbed of human-autonomous vehicle interactions. The human is monitored by a number of sensors, and interacts with the driving simulator through the PreScan software interface.)} 
% \label{fig:testbed}
% \end{figure}
%These are systems with sensors that monitor the vehicle's surroundings and that use the acquired information to take action. Such actions may range from warning the driver of a potentially dangerous situation to actively evading hazards by means of automatic braking or automatic steering. 
% The ``human monitoring and sensing'' block in Figure~\ref{fig:testbed} encloses the sensors that will be used for both high-level inference of human's intent and preferences and low-level monitoring of human behavioral, mental and physiological states. These sensors include EEG for neural signals, EKG for heart activity, EMG for muscle activity, a camera for head tracking, eye tracking suite and cloud-based speech recognizer.

\subsection{Test-beds for reachability and control synthesis experiments}
%\nicola{Nicola describes the experiment setup for reachability analysis experimentation}
% \begin{wrapfigure}{R}{0.35\linewidth}
% \vspace{-5pt}
% \centering
% \includegraphics[width = \linewidth ]{figures/jackal_2.jpg} 
% \caption{one of the autonomous ground vehicles available in PI Bezzo's lab to validate the proposed research.}
% \vspace{-5pt}
% \label{fig:jackal}\end{wrapfigure}
%\subsubsection{Autonomous ground vehicles}
The proposed research, in Thrust 2 about safety assessment via reachability analysis and and reconfiguration via safe reinforcement learning will be validated using our testbed of autonomous ground vehicle available in Co-PI Bezzo's robotics laboratory. 
Figure~\ref{fig:all_experiments} shows the autonomous ground vehicle equipped with the same sensors available in real autonomous cars which include velodyne lidar, stero cameras, IMUs, wheel encoders, differential GPS, and on-board i7 cpu. The vehicle's equations of motion and dynamics are similar to real autonomous vehicles but at lower speeds. This platform will be used to create a library of primitives for the reachability analysis and then used at run-time to predict and validate the safety assessment presented in Thrust 2. 
%We also plan to use virtual reality goggles (also available in the PI's lab) to simulate the presence of a human on board the vehicle. 
\begin{figure}
    \centering
    \includegraphics[width=0.65\columnwidth]{figures/tesdbed_all.pdf}
    \caption{Automotive Cyber-physical System (CPS) test-beds which will be used for conducting the experimentation to support the proposed research. In addition, we will also utilize a real car to gather driving profile data as describe in Thrust 1. }
    \label{fig:all_experiments}
\end{figure}
PI Behl has developed a 1/10 scale autonomous racing car platform called F1/10~\cite{f1tenth} (Figure~\ref{fig:all_experiments}. The cars are a realistic representation of a full scale car, with similar dynamics, just different parameters. %It carries a suite of sensors similar to that carrier by current full scale prototypes of autonomous vehicles, including a LIDAR, a stereo camera, depth camera, an IMU, and a high performance GPU board running the Robot Operating System (ROS).
PI Behl has built a fleet of F1/10 cars and has access to an indoor track facility for testing the control synthesis algorithms in a safe environment but at high and realistic traffic speeds. The PI also has access to a 1/10 scale car which has been modified in that it can be driven in first person view (FPV) using steering wheels and pedals, while wearing a headset which relays the onboard camera view to the human driver, thereby allowing us to conduct both UI and driving style behavior experiments at 1/10 the scale.

