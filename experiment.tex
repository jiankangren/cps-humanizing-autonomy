%!TEX root = main.tex

\section{Experimentation and Evaluation Plan}
\label{sec:experiment}

% \begin{itemize}
%     \item We need to describe in detail all the experiments and testbeds we will use/develop.
%     \item this is emphasized int he solitication
%     \item describe how we will recruit subjects
%     \item emphasize the real world experimentation plan
% \end{itemize}

% \textit{This section should describe how the research concepts proposed will be demonstrated and validated. It should present metrics for success. It should identify critical experiments, and describe how the research will be demonstrated, including through simulation, prototyping, and integration with real (including sub-scale) cyber-physical systems. For Medium and Frontier projects, the validation plan must include experimentation on an actual cyber-physical system.}

%\lu{Lu describes the full scale driving simulator}


Each of the proposed research thrusts in Sections~\ref{sec:behaviour},\ref{sec:reachability}, and \ref{sec:feedback}, we will conduct extensive experimentation using high fidelity automobile simulators like PreScan and AirSim, a full scale diving simulator, smaller scale autonomous vehicle test-beds, and a full scale car fitted with sensors for monitoring human driving behavior and emotions. We next describe in detail, the different cyber-physical systems tesbeds, and a cross cutting experimentation plan for the three research thrusts. 


\subsection{Scenario-based behavior and trust modeling}
\label{subsec:trust-modeling}
\input{scenarios}




% \begin{figure}[t]
% \centering
% \includegraphics[width=.9\textwidth]{figures/testbed}
% \caption{Our partially built testbed of human-autonomous vehicle interactions. The human is monitored by a number of sensors, and interacts with the driving simulator through the PreScan software interface.)} 
% \label{fig:testbed}
% \end{figure}

 %These are systems with sensors that monitor the vehicle's surroundings and that use the acquired information to take action. Such actions may range from warning the driver of a potentially dangerous situation to actively evading hazards by means of automatic braking or automatic steering. 
% The ``human monitoring and sensing'' block in Figure~\ref{fig:testbed} encloses the sensors that will be used for both high-level inference of human's intent and preferences and low-level monitoring of human behavioral, mental and physiological states. These sensors include EEG for neural signals, EKG for heart activity, EMG for muscle activity, a camera for head tracking, eye tracking suite and cloud-based speech recognizer.

\subsection{Test-beds for reachability and control synthesis experiments}

%\nicola{Nicola describes the experiment setup for reachability analysis experimentation}

\begin{wrapfigure}{R}{0.35\linewidth}
\vspace{-5pt}
\centering
\includegraphics[width = \linewidth ]{figures/jackal_2.jpg} 
\caption{one of the autonomous ground vehicles available in PI Bezzo's lab to validate the proposed research.}
\vspace{-5pt}
\label{fig:jackal}\end{wrapfigure}

\subsubsection{Autonomous ground vehicles}
The proposed research, in particular Thrust 2 about safety assessment via reachability analysis and and reconfiguration via safe reinforcement learning will be validated using our testbed of autonomous ground vehicle available in PI Bezzo's robotics laboratory. Figure~\ref{fig:jackal} shows one of the autonomous ground vehicle equipped with the same sensors available in real autonomous cars which include velodyne lidar, stero cameras, imus, wheel encoders, differential GPS, and onboard i7 cpu. The vehicle's equations of motion and dynamics are similar to real autonomous vehicles with reduced speed. This platform and especially the sensor testbed will be used offline to create a library of primitives for the reachability analaysis and then used at runtime in different scenarios to predict and validate the safety assessment presented in Thrust 2. We also plan to use virtual reality goggles (also available in the PI's lab) to simulate the presence of a human on board the vehicle. 

\subsubsection{1/10 scale autonomous vehicle testbed}

PI Behl has developed a 1/10 scale autonomous racing car platform called F1/10~\citeN{f1tenth}. The cars are a realistic representaiton of a full scale car, with similar dynamics, just different parameters. It carries a suite of sensors similar to that carrier by current full scale prototypes of autonomous vehicles, including a LIDAR, a stereo camera, depth camera, an IMU, and a high performance GPU board running the Robot Operating System (ROS).
PI Behl has built a fleet of F1/10 cars and has access to an indoor track facility for testing the control synthesis algorithms in a safe environment but at high and realistic traffic speeds. The PI also has access to a 1/10 scale car which has been modified in that it can be driven in first person view (FPV) using steering wheels and pedals, while wearing a headset which relays the onboard camera view to the human driver, thereby allowing us to conduct both UI and driving style behavior experiments at 1/10 the scale.



%\madhur{Madhur describes F1/10 - could be used for reachibility + control synthesis}