\subsection{Research Thrust 1: Behavior modeling}
\label{sec:behaviour}

 The ability to (1) detect, (2) assess and (3) manage a person’s emotions has been identified to be the predictor of success in relating to the people around us. By being able to read other’s “emotional cues,” not only we can better understand how they are feeling at a given time, but it also helps us to predict how they will respond in different scenarios. Research suggests that emotions are normally associated with specific events or occurrences (cite), and they can significantly influence our thoughts and behaviors. Additionally, we can use reason to evaluate our emotions, interpret them, and reassess our initial reactions to them. Therefore, by detecting certain events and occurrences, we may be able to assess and predict individual emotional states and use reason to soften their impact or “shift” their meaning. Building on this knowledge, to enhance driver/passenger(s) experience, safety, and trust in the AV, future AVs must be automatically and accurately detect, assess, and manage the passengers behaviors and emotions. Through this approach AVs can better collaborate and communicate with the passengers and develop “human-like” trust with them. 
 
  Within the past two decades, a number of Naturalistic Driving Studies (NDS) aimed at understanding human behaviors in different safety-related conditions, with the specific emphasis towards crash or near-crash incidents. In these studies, cars were equipped with sensors and devices that continuously monitor various aspects of driving behavior such as vehicle acceleration, deceleration, location, and speed, driver-related information such as eye, head and hand movement, and environmental conditions such as traffic and weather conditions (CITE). For instance, in (CITE - European paper) researchers identified that certain driver-personality traits are prone to committing speeding violations; or bad behaviors may cluster together, where drivers who perform one type of risky behavior are more likely to engage in other types of risky behaviors. Although these studies provide significant insights on the driver behavior in different scenarios and contextual settings, they specifically focus only on safety-related behaviors that result in crashes or near-crash incidents; additionally, the first generation of NDS used manual video annotation techniques to identify driver behaviors in specific epochs of driving (CITE), which requires extensive time and manpower to complete. Due to complexities related to human behaviors (i.e., dynamic changes in human behaviors) many of the first generation NDS were limited to short-term monitoring and as a result, lacked in identifying how the driver behaviors may change based on various contextual, environmental, and social settings. With the growing interest in long-term behavioral monitoring and automated monitoring approaches, in more recent NDS studies, researchers are focusing on large-scale computer vision-based and audio processing analyses of human behavior where information can  semi-automatically and/or automatically be extracted from raw video and audio feeds (CITE). For instance, in a recent study from MIT, researchers are using computer vision and voice-processing techniques to monitor drivers body posture, eye movement, and emotions to predict driver's frustration and gaze region that may result in unsafe behaviors while driving semi-automated and fully-automated vehicles (CITE behavioral impact of drivers roles and other MIT work). 
  
    As our cars are moving towards higher levels of autonomy, behavioral and emotional information not only are extremely useful for increasing safety of AVs, they can also be applied towards optimizing control actions of AVs, and most importantly, enhancing the  driving-experience for the driver and/or passenger(s); for instance, by identifying that the driver is feeling “happy,” AVs can optimize their route selection in order to increase the drivers positive emotions, potentially enhancing the driver's mental health. Additionally, by optimizing the decision making of automated systems around the user's specific preferential and comfort profiles, research suggests that the automated system can better gain user trust (cite). Although the more recent NDS are implementing automated monitoring techniques for detecting driver behaviors and emotions, they specifically focus on identifying safety-related behaviors in semi and fully-automated vehicles. As a result, there exists a lack of  understanding on the causation of behaviors as a collective interactions of environmental, emotional, physiological, and social factors (\textit{Gap 1}); and integration of behavioral models into AV safety and control actions(\textit{Gap 2}). Additionally, within the exiting work, limited number of studies have investigated the impacts of social interactions on the driver and passengers behaviors and emotions; by being able to identify the impacts of social interactions on certain behaviors, AVs can better optimize the vehicle's level of autonomy according to the behavioral and emotional states of the driver as well. 

  %current studies also lack real-time data processing.
  
  In this project, we aim to understand how environmental, physiological, and social factors may influence and/or cause certain behaviors and emotions in the driver and passengers. Through instrumenting a number of participant-owned vehicles, we will collect and analyze the impact of different factors on driver/passenger behaviors and emotions. To collect environmental-related information, Automatic Pro (CITE) will be used to monitor the car's speed and location. Automatic Pro will be also used to collect behavioral information such as driver's acceleration and deceleration rates (previous research suggests acceleration/deceleration can be used to identify risky or frustrated drivers (CITE)). Cameras will be used to capture outdoor conditions such as traffic, weather, surrounding cars and pedestrians, signs, infrastructure, etc. as well as indoor conditions such as social interactions, drivers and passenger(s)'s body posture, face, head and hand/arm movement. Ambient-condition sensors will be used to measure thermal settings, indoor and outdoor noise levels, and lighting levels. Audio recorders will be used to capture social interactions as well as any other audio sources (i.e., music, radio, audio books, etc.). Wearables such as Samsung Gear or Fitbit will be used to measure the driver and passengers physiological data (i.e., heart rate and hand/arm movement). Through face recognition software (i.e., OpenSmile, iMotion, and Google API) recorded videos will be processed to create a time-stamped emotional states for the driver and passengers; additionally, audio data will be processed through speech-to-tech and natural language processing techniques to identify and classify various social interactions and audio sources (e.g., music or radio). The audio information as well as physiological information can also be used to identify the driver/passengers emotional states. \ref{fig:sensors} provides an overview of the sensors and data that will be collected from the drivers and passenger(s).   

 To address \textit{Gap 1}, we plan to observe trends in our data that both reveal (1) factors that influence specific human emotions and (2) the downstream consequences of particular emotions on their behaviors. For example, we will aim to understand what environmental (e.g., weather, thermal conditions, lighting, noise) and social factors (e.g., social interaction), physiological factors (e.g.,arm movement) trigger particular emotions. Through this approach, we will also be able to develop a database of  emotions, behaviors and environmental changes. As it may be difficult to often identify triggers to particular emotions (i.e., passenger may appear already experiencing a particular emotion), we plan to evaluate which environmental (indoor and outdoor) and social factors may be used as cues for particular emotions and/or behaviors. For example, as our preliminary data indicates, in rainy conditions, a driver focuses more on his driving, and as a result his acceleration/declaration rates are dropped compared to normal conditions; meanwhile the driver is more likely to not listen to any music. With this information, we will then assess if we can reliably predict particular emotions or behaviors, given specific contextual, environmental or social factor(s); for instance in a rainy condition, the driver will have higher control of the vehicle.The specific research questions that will be addressed through this approach include: 
\begin{enumerate}[itemsep=0pt,parsep=0pt,topsep=4pt,leftmargin=0.4in]
    \item What is the impact of social interaction on driving behavior? 
    \item What is the taxonomy of driving behaviors and emotions based on environmental conditions, social interactions, and physiological changes?
    \item How can driving behavior and emotions be non-intrusively and with least number of sensors be detected?
    \item What are environmental and social factors that impact passenger(s) attention (positively or negatively)? 
    \item Do people have differentiating “trust” profiles according to specific behaviors and contextual conditions?  

\end{enumerate}
 
  By having an understanding about the causation of certain behaviors and emotions (e.g., impact of weather conditions on driving-behavior), behavioral and emotional models can be developed and integrated into AVs systems where they can optimize the control actions as well as feedback/communication strategies based on the driver preferences and behaviors. To achieve this, we propose to develop an Artificial Intelligence (AI) engine that aggregates the information extracted from vision and audio with physiological and environmental information. Through clustering techniques and machine learning algorithms (i.e., CNL and HMM), we will define behavioral models as a function of emotional traits, environmental conditions, social interactions, and physiological measures.%Madhur to improve
  As a result address \textit{Gap 2}, we are specifically interested in formally characterizing emotions and behaviors through a “context-dependence” approach, where spatial and temporal information can be automatically detected, analyzed, and interpreted. Through this approach, behavioral and emotional models can be generated and applied towards:
 \begin{enumerate}[itemsep=0pt,parsep=0pt,topsep=4pt,leftmargin=0.4in]
     \item determining the safety regions according to the reach-ability analysis in Thrust 2 (Section~\ref{sec:reachability}),
     \item as input to the constraints on the control actions to select a trajectory which conforms to the users behavior needs in Thrust 2 (Section~\ref{sec:reachability})
     \item a detection mechanism which triggers when the natural language explanation of the cars actions is generated Thrust 3 (Section~\ref{sec:feedback})
 \end{enumerate}

 
\begin{figure}
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/sensors.png}
    \caption{Overview of instrumentation of vehicles and the collected information from participants}
    \label{fig:sensors}
\end{figure}
 
 


%\begin{center}
    %\begin{tabular}{ | l | l | l |}
    %\hline
    %Type &  Factors & Sensor/Algorithms \\ \hline
    %\multirow{5}{*}{Environmental Sensing} & Speed & Automatic Pro \\
        %& Location & GPS/Automatic Pro \\
       % & indoor and outdoor conditions & cameras \\
      %  & Passengers' identity and count & cameras \\
     %   & Weather Conditions & Camera and weather database\\\hline
    %\multirow{8}{*}{Human Sensing} & Face features & eye tracking, camera, facial recognition \\
       % & Noise levels & noise-level sensors \\
       % & Passenger voice & Voice recorder, Natural Language Processing \\
      %  & Brake and acceleration rate & Automatic Pro \\
     %   & Grip on steering wheel & pressure sensor \\
    %    & Music and other social media use & APIs (e.g., Spotify) \\
   %     & Social interaction & Camera and voice recorder \\ \hline
  %  
 %   \end{tabular}
%\end{center}








